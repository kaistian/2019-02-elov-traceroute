{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "from shapely_geojson import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hvor går trafikken?\n",
    "Denne notebooken henter ned de største domenene fra [Tranco](https://tranco-list.eu/list/MW69/1000000), filtrerer ut alle domener som ikke ender på .no. Da sitter man igjen med en liste over 1343 norske domener. Disse domenene kjøres så gjennom traceroute for å finne ut hvilke IPer trafikken går gjennom på veien. Vi sjekker så hvor serverne til disse IPene befinner seg gjennom GeoIP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranco\n",
    "Listen kan lastes ned her (den ligger også som en .csv i dette repoet): https://tranco-list.eu/list/MW69/1000000\n",
    "\n",
    "Listen er en sammenstilling av toppdomenene fra Alexa, Umbrella, Majestic og Quantcast fra 2018-12-18 til 2019-01-16 (30 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tranco = pd.read_csv('tranco_MW69.csv', names=\"domain\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tranco = set(tranco[tranco.domain.str.endswith('.no')].domain.str.replace('www.', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tranco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceroute-hjelpefunksjoner\n",
    "Denne notebooken kjøres både av en Windows-maskin og en Mac, så vi har laget en liten funksjon som abstraherer traceroute-kjøringen. Vi har også en liten hjelpefunksjon for å kjøre IPer gjennom Dazzlepod.com for IP-lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_site(site_name):\n",
    "    proc_name = \"traceroute\"\n",
    "    if os.name == \"nt\":\n",
    "        proc_name = \"tracert\"\n",
    "        s = subprocess.Popen([proc_name, '-h', '16', '-w', '1000', site_name], stdout=subprocess.PIPE)\n",
    "    else: #UNIX\n",
    "        s = subprocess.Popen([proc_name, '-n', '-w', '0.5', '-q', '1', '-m', '16', site_name], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return s.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ip(ip):\n",
    "    r = requests.get('https://dazzlepod.com/ip/'+ip, headers={'User-Agent': 'curl'}).json()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceroute-kjøring\n",
    "**MERK!** Denne kjøringen tar lang tid. Hvis du bare vil leke med kjøringen vi har gjort, gå to bolker ned og kommenter ut `df`-deklarasjonen som laster inn CSV-filen vi har generert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nettsider = {}\n",
    "n = 0\n",
    "\n",
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "print (\"Start time:\", localtime)\n",
    "\n",
    "if os.name == \"nt\": # win - Martin\n",
    "\n",
    "    print('Using Windows')\n",
    "    for nettside in tranco:\n",
    "        hop = 1\n",
    "        \n",
    "        print(f\"Looking up: {nettside} ({n})\")\n",
    "        n += 1\n",
    "        data = lookup_site(nettside)\n",
    "        data = data[0].decode('ascii')\n",
    "        ips = []\n",
    "        \n",
    "#        print(f\"data: {data}\")\n",
    "\n",
    "        for i, obj in enumerate(data.split('\\r\\n')):\n",
    "\n",
    "#            print(f\"obj: {obj}\")\n",
    "\n",
    "            if  obj.find('Tracing route') != -1 or obj.find('net unreachable') != -1 or obj.find('over a') != -1 or obj.find('Unable to resolve') != -1:\n",
    "#                print(\"if none\")\n",
    "                None\n",
    "\n",
    "            elif '*' in obj or len(obj.split('ms ')) < 4:\n",
    "                #removes shorter lines and unresolved\n",
    "#                print(\"elif none\")\n",
    "                None \n",
    "            \n",
    "            elif obj.find('[') != -1: \n",
    "#                print(f\"obj: {obj}\")   \n",
    "                ips.append((hop, obj.split(\"[\")[1].strip(\"] \")))\n",
    "                hop = hop + 1 \n",
    "#                print(f\"'elif append {ips}\")\n",
    "            \n",
    "            elif len(obj.split('ms ')) == 4:\n",
    "                ips.append((hop, obj.split(\" ms \")[3].strip()))\n",
    "                hop = hop + 1\n",
    "#                print(f\"'else append {ips}\")\n",
    "        df_nettsider[nettside] = ips\n",
    "    \n",
    "else: \n",
    "        # Henrik\n",
    "    print('Using UNIX')\n",
    "    for nettside in nettsider:\n",
    "        print(\"Looking up\", nettside)\n",
    "        data = lookup_site(nettside)\n",
    "        data = data[0].decode('ascii')\n",
    "        ips = []\n",
    "        for i, obj in enumerate(data.split('\\n')):\n",
    "            if '*' in obj or len(obj.split(' ')) < 3:\n",
    "                break\n",
    "            ips.append((i+1, obj.split(' ')[3]))\n",
    "\n",
    "        df_nettsider[nettside] = ips   \n",
    "        \n",
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "print (\"End time:\", localtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates a geojson and a dataframe\n",
    "geo_hops = []\n",
    "features = {}\n",
    "df_geo = []\n",
    "\n",
    "tmptf = time.localtime()\n",
    "\n",
    "for nettside, hops in df_nettsider.items():\n",
    "    obj = {'website': nettside }\n",
    "    features[nettside] = []\n",
    "\n",
    "    for hop, ip in hops:\n",
    "        got_ip = get_ip(ip)\n",
    "        lat = 0\n",
    "        lng = 0\n",
    "        country = 'NONE'\n",
    "        if 'longitude' in got_ip and 'latitude' in got_ip and 'country_code' in got_ip:\n",
    "            lat = got_ip['latitude']\n",
    "            lng = got_ip['longitude']\n",
    "            country = got_ip['country_code']\n",
    "\n",
    "            features[nettside].append(Feature(Point(lng, lat), {'country': country, 'hop': hop, 'website': nettside}))\n",
    "            df_geo.append([nettside, hop, ip, country, lat, lng])\n",
    "            \n",
    "df = pd.DataFrame(df_geo, columns=['website', 'hop', 'ip','country','lat','lgn'])     \n",
    "\n",
    "df.to_csv('tranco-%d-%02d-%02d.csv' % (tmptf.tm_year, tmptf.tm_mon, tmptf.tm_mday), index=False, encoding='utf-8')\n",
    "\n",
    "feature_total = []\n",
    "for nettside, feature_list in features.items():\n",
    "    feature_total.extend(feature_list)\n",
    "_featurecol2 = FeatureCollection(feature_total)\n",
    "with open('tranco-%d-%02d-%02d.geojson' % (tmptf.tm_year, tmptf.tm_mon, tmptf.tm_mday), 'w') as fp:\n",
    "    fp.write(dumps(_featurecol2, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kommentér ut disse for å bruke den pregenererte CSV-filen fra NRKbeta\n",
    "#df = pd.read_csv('tranco-190124.csv')\n",
    "#df = tmpdf[tmpdf.website.str.endswith('.no')]\n",
    "#df.to_csv('tranco-190124.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "websites = df.groupby(['website', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foreign_hops = len(websites.filter(lambda x: 'NO' not in x.country.unique()).website.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Antall nettsider hvor minst ett hopp går utenfor Norge: %d\" % foreign_hops)\n",
    "print(\"Prosentandel av nettsider hvor minst ett hopp går utenfor Norge: %f\" % (foreign_hops/len(df.website.unique())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
